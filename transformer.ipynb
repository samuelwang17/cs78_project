{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grad_skip_softmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sm(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # skip gradient through the softmax on backward pass\n",
    "        return grad\n",
    "        \n",
    "class gru(nn.Module):\n",
    "    # 'gated-recurrent-unit type gating' as seen in GTrXL paper\n",
    "    def __init__(self, dim, b_g = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_r = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_r = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.w_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.u_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.b_g = b_g # this is used to hack initial bias of the above to be below zero, such that gate is initialized close to identity\n",
    "        self.w_g = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_g = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        r = self.sigmoid(self.w_r(y) + self.u_r(x))\n",
    "        z = self.sigmoid(self.w_z(y) + self.u_z(x) - self.b_g) # when zero, gate passes identity of residual\n",
    "        h_hat = self.tanh(self.w_g(y) + self.u_g(r * x))\n",
    "        g = (1-z)*x + z * h_hat\n",
    "        return g\n",
    "        \n",
    "\n",
    "class mlp(nn.Module):\n",
    "    # 1d temporal convolution\n",
    "    # no communication between tokens, uses same kernel for each token spot\n",
    "    def __init__(self, embed_dim, internal_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(embed_dim, internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(internal_dim, embed_dim)\n",
    "        ) # no second relu at output of mlp\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.block(input)\n",
    "\n",
    "\n",
    "class cross_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "\n",
    "        return self.attention(x, enc, enc)[0]\n",
    "\n",
    "class mha_helper(nn.Module):\n",
    "    def __init__(self,\n",
    "    dim_model,\n",
    "    heads,\n",
    "    bias: bool = False,\n",
    "    smeared: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_m = dim_model\n",
    "        self.heads = heads\n",
    "        self.d_k = dim_model // heads\n",
    "\n",
    "        self.affine = nn.Linear(\n",
    "            in_features = dim_model,\n",
    "            out_features = heads * self.d_k,\n",
    "            bias = bias\n",
    "            )\n",
    "        \n",
    "        # key smear functionality\n",
    "        self.smeared = smeared\n",
    "        self.previous  = None\n",
    "        self.alpha = torch.tensor([1.], requires_grad=True) # learned interpolation parameter, initialized to 1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        shape = x.shape[:-1]\n",
    "        x = self.affine(x)\n",
    "\n",
    "        if self.smeared:\n",
    "            itpl = self.sigmoid(self.alpha) # interpolation value\n",
    "            x = itpl * x + (1 - itpl) * self.previous if (self.previous != None) else x\n",
    "            self.previous = x\n",
    "        x = x.view(*shape, self.heads, self.d_k)\n",
    "        return x\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    # smeared key self attention\n",
    "    # consider adding stop grad memory\n",
    "    def __init__(self,\n",
    "    dim_model,\n",
    "    heads,\n",
    "    sequence_length,\n",
    "    masked = False,\n",
    "    smeared_key = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.masked = masked\n",
    "        self.heads = heads\n",
    "        self.q = mha_helper(dim_model, heads)\n",
    "        self.k = mha_helper(dim_model, heads, smeared=smeared_key)\n",
    "        self.v = mha_helper(dim_model, heads, bias=True)\n",
    "        self.scaler = 1 / math.sqrt(float(dim_model // heads))\n",
    "        self.softmax = nn.Softmax(1) # transform to stochastic matrix -- rows sum to one\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_model),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.smeared_key = smeared_key\n",
    "        self.mask = torch.tril(torch.ones(sequence_length,sequence_length)).unsqueeze(-1)\n",
    "\n",
    "    def mask_format(self, mask: torch.Tensor, query_shape: list[int], key_shape: list[int]):\n",
    "        assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n",
    "        assert mask.shape[1] == key_shape[0]\n",
    "        assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        return mask\n",
    "        \n",
    "    def attn_score(self, q, k):\n",
    "        return torch.einsum('ibhd, jbhd -> ijbh', q, k) # dot product attention\n",
    "        # d is supposed to disappear, so must be heads/dim, h probably means heads, b is probably batch then, so i and j should be sequence sequence\n",
    "        # i  is batch, j is batch for other, h is heads, d is heads over dim, b is sequence\n",
    "        # batch batch sequence heads\n",
    "        # should be batch, sequence, sequence, \n",
    "        #key is batch, sequence, heads, heads over him\n",
    "\n",
    "    def forward(self, input):\n",
    "        length, batch_size, _ = input.shape\n",
    "        q = self.q(input)\n",
    "        k = self.k(input)\n",
    "        v = self.v(input)\n",
    "        \n",
    "        scores = self.attn_score(q,k) * self.scaler\n",
    "\n",
    "        if self.masked:\n",
    "            mask = self.mask_format(self.mask, q.shape, k.shape)\n",
    "            masked_scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        else:\n",
    "            masked_scores = scores\n",
    "        attn = self.softmax(masked_scores)\n",
    "\n",
    "        x = torch.einsum('ijbh,jbhd->ibhd', attn, v)\n",
    "\n",
    "        x = x.reshape(length, batch_size, -1)\n",
    "\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # not masked, no cross attention, no memory, for encoder\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = self_attention(\n",
    "            dim_model=embed_dim,\n",
    "            heads=attention_heads,\n",
    "            sequence_length=sequence_length,\n",
    "            masked = False,\n",
    "            smeared_key = False\n",
    "        )\n",
    "        self.mlp = mlp(\n",
    "            embed_dim= embed_dim,\n",
    "            internal_dim=mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class decoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # masked, cross attention, no memory, for decoder core layers\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_lenth,\n",
    "    smeared_key = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = self_attention(\n",
    "            dim_model = embed_dim,\n",
    "            heads = attention_heads,\n",
    "            sequence_length = sequence_lenth,\n",
    "            masked = True,\n",
    "            smeared_key = smeared_key\n",
    "        )\n",
    "\n",
    "        self.cross_mha = cross_attention(\n",
    "            embed_dimension = embed_dim,\n",
    "            num_heads = attention_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim = embed_dim,\n",
    "            internal_dim = mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate3 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "        # masked self attention\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "\n",
    "        # cross attention\n",
    "        # consider output sequence length and \n",
    "        y = self.ln1(x)\n",
    "        enc = self.ln1(enc)\n",
    "        y = self.cross_mha(enc, x)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "\n",
    "        # position-wise multi layper perceptron\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # no inductive biases on encoder here\n",
    "        self.block = nn.Sequential()\n",
    "        for x in range(layers):\n",
    "            self.block.append(encoder_layer(\n",
    "                embed_dim = model_dim,\n",
    "                mlp_dim = mlp_dim,\n",
    "                attention_heads = heads,\n",
    "                sequence_length = sequence_length\n",
    "            ))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        first_layer = decoder_layer(\n",
    "            embed_dim = model_dim,\n",
    "            mlp_dim = mlp_dim,\n",
    "            attention_heads = heads,\n",
    "            sequence_lenth = sequence_length,\n",
    "            smeared_key = True\n",
    "        )\n",
    "\n",
    "        self.block = [first_layer]\n",
    "\n",
    "        for x in range(layers - 1):\n",
    "            self.block.append(\n",
    "                decoder_layer(\n",
    "                    embed_dim = model_dim,\n",
    "                    mlp_dim= mlp_dim,\n",
    "                    attention_heads= heads,\n",
    "                    sequence_lenth = sequence_length\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # y is input from encoder\n",
    "        for layer in self.block:\n",
    "            x = layer(x,y)\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    attn_heads,\n",
    "    sequence_length,\n",
    "    enc_layers,\n",
    "    dec_layers,\n",
    "    action_dim\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder(\n",
    "            layers=enc_layers,\n",
    "            model_dim=model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length = sequence_length\n",
    "        )\n",
    "\n",
    "        self.decoder = decoder(\n",
    "            layers=dec_layers,\n",
    "            model_dim= model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length=sequence_length,\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(model_dim, action_dim),\n",
    "            nn.ReLU(),\n",
    "            grad_skip_softmax() # To do neural replicator dynamics\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(model_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        enc = self.encoder(enc_input)\n",
    "        dec = self.decoder(dec_input, enc)\n",
    "        policy = self.actor(dec)\n",
    "        value = self.critic(dec)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrl = RLformer(\n",
    "    model_dim = 10,\n",
    "    mlp_dim = 20,\n",
    "    attn_heads = 2,\n",
    "    sequence_length = 15,\n",
    "    enc_layers = 2,\n",
    "    dec_layers = 2,\n",
    "    action_dim = 4 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input = torch.rand((15,8,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = torch.rand((15,8,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 8, 10])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/0v1g6ztj2_3272nwsptb6rpc0000gn/T/ipykernel_16511/3787764247.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.sm(x)\n"
     ]
    }
   ],
   "source": [
    "policy, value = myrl(enc_input, dec_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 8, 4])"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 8, 1])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
