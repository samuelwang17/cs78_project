{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np\n",
    "from transformer import mod_transformer as RLformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self,\n",
    "        model_dim,\n",
    "        mlp_dim,\n",
    "        attn_heads,\n",
    "        sequence_length,\n",
    "        enc_layers,\n",
    "        dec_layers,\n",
    "        action_dim,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = RLformer(\n",
    "            model_dim = model_dim,\n",
    "            mlp_dim = mlp_dim,\n",
    "            attn_heads = attn_heads,\n",
    "            sequence_length = sequence_length,\n",
    "            enc_layers = enc_layers,\n",
    "            dec_layers = dec_layers,\n",
    "            action_dim = action_dim,\n",
    "        )\n",
    "\n",
    "        self.tokenizer =  Tokenizer(model_dim=model_dim)\n",
    "    \n",
    "    def init_player(self, player, hand):\n",
    "        # initialize this players hand and tokenize it, store it in buffer\n",
    "        hand_tensor = Tokenizer(hand) #hand needs to be card observations -- list of length two of tensors\n",
    "        self.register_buffer(f'hand_{player}', tensor= hand_tensor)\n",
    "\n",
    "    def forward(self, player, obs_flat):\n",
    "        #takes flattened inputs in list form, not tokenized\n",
    "        enc_input = self.get_buffer(f'hand_{player}')\n",
    "        dec_input = Tokenizer(obs_flat)\n",
    "        policy, value = self.model(enc_input, dec_input)\n",
    "\n",
    "        return policy, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokerenv import poker_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class actor_critic():\n",
    "    #Needs to be able to run hand, return loss with grad enabled\n",
    "    def __init__(self, \n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    enc_layers,\n",
    "    dec_layers,\n",
    "    max_sequence: int = 200, \n",
    "    n_players: int = 2,\n",
    "    gamma: float = .8,\n",
    "    n_actions: int = 10, # random placeholder value\n",
    "    ) -> None:\n",
    "        self.gamma = gamma\n",
    "        self.env = poker_env(n_players = n_players)\n",
    "        self.agent = Agent(\n",
    "            model_dim = model_dim,\n",
    "            mlp_dim = mlp_dim,\n",
    "            attn_head = heads,\n",
    "            sequence_length = max_sequence,\n",
    "            enc_layers = enc_layers,\n",
    "            dec_layers = dec_layers,\n",
    "            action_dim = n_actions,\n",
    "        )\n",
    "\n",
    "        self.observations = [] #this will be a list of lists, each is the list of observations in a hand\n",
    "        self.obs_flat = list(chain(*self.observations))\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.rewards_flat = list(chain(*self.rewards))\n",
    "\n",
    "        self.values = []\n",
    "        self.val_flat = list(chain(*self.values))\n",
    "\n",
    "        self.action_log_probabilies = []\n",
    "        self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "        self.max_sequence = max_sequence\n",
    "\n",
    "        self.n_players = n_players\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.detokenize = None #detokenizer HERE\n",
    "\n",
    "    def init_hands(self):\n",
    "        # get all hands\n",
    "        # run encoder for each of players\n",
    "        for player in range(self.n_players):\n",
    "            hand = self.env.get_hand(player)\n",
    "            self.agent.init_player(player, hand)\n",
    "    \n",
    "    def chop_seq(self):\n",
    "        #if length of observations is above a certain size, chop it back down to under sequence length by removing oldest hand\n",
    "        #return flattened version to give to model on next run\n",
    "        if len(self.observations) > self.max_sequence:\n",
    "            self.observations = self.observations[1:]\n",
    "            self.obs_flat = list(chain(*self.observations))\n",
    "\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.rewards_flat = list(chain(*self.rewards_flat))\n",
    "\n",
    "            self.values = self.values[1:]\n",
    "            self.val_flat = list(chain(*self.values))\n",
    "\n",
    "            self.action_log_probabilies = self.action_log_probabilies[1:]\n",
    "            self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "        else:\n",
    "            self.obs_flat = list(chain(*self.observations))\n",
    "            self.rewards_flat = list(chain(*self.rewards_flat))\n",
    "            self.val_flat = list(chain(*self.values))\n",
    "            self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "    def play_hand(self):\n",
    "        # makes agent play one hand\n",
    "        \n",
    "        # deal cards\n",
    "        rewards, observations = self.env.new_hand() # start a new hand\n",
    "        self.init_hands() # pre load all of the hands\n",
    "\n",
    "        # init lists for this hand\n",
    "        self.observations += [observations] \n",
    "        self.rewards += [rewards]\n",
    "\n",
    "        self.chop_seq() # prepare for input to model\n",
    "        \n",
    "        hand_over = False\n",
    "        while not hand_over:                \n",
    "\n",
    "            # get values and policy -- should be in list form over sequence length\n",
    "            policy, values = self.agent(self.obs_flat)\n",
    "            value = values[-1].detach().numpy()[0,0] # get last value estimate\n",
    "            dist = policy[-1].detach().numpy() # get last policy distribution\n",
    "\n",
    "            # randomly sample an action\n",
    "            action = np.random.choice(self.n_actions, p=np.squeeze(dist))\n",
    "\n",
    "            # UNFINISHED: Need to detokenize actions HERE\n",
    "            action = self.detokenize(action)\n",
    "\n",
    "            alp = torch.log(policy.squeeze(0)[action])\n",
    "            reward, obs, hand_over = self.env.take_action(action) # need to change environment to return hand_over boolean\n",
    "\n",
    "            # add new information from this step\n",
    "            self.rewards[-1].append(reward)\n",
    "            self.observations[-1].append(obs)\n",
    "            self.values[-1].append(value)\n",
    "            self.action_log_probabilies.append(alp)\n",
    "            \n",
    "            # prepare for next action\n",
    "            self.chop_seq()\n",
    "        \n",
    "        V_T, _ = self.agent(self.obs_flat)\n",
    "        \n",
    "        # process gradients and return loss:\n",
    "        return self.get_loss(V_T)\n",
    "\n",
    "    def get_loss(self, values, rewards, V_T):\n",
    "\n",
    "        Qs = []\n",
    "        Q_t = V_T\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Q_t = rewards[t] + self.gamma * Q_t\n",
    "            Qs[t] = Q_t\n",
    "        \n",
    "        Qs = torch.FloatTensor(Qs)\n",
    "        values = torch.FloatTensor(self.val_flat)\n",
    "        alps = torch.stack(self.alp_flat)\n",
    "        advantages = Qs - values\n",
    "\n",
    "        \n",
    "        actor_loss = (-alps * advantages).mean() # loss function for policy going into softmax on backpass\n",
    "        critic_loss = 0.5 * advantages.pow(2).mean() # autogressive critic loss - MSE\n",
    "        loss = actor_loss + critic_loss # no entropy in this since that would deviate from deepnash\n",
    "        return loss\n",
    "    \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
