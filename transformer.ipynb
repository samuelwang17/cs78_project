{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grad_skip_softmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sm(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # skip gradient through the softmax on backward pass\n",
    "        return grad\n",
    "        \n",
    "class gru(nn.Module):\n",
    "    # 'gated-recurrent-unit type gating' as seen in GTrXL paper\n",
    "    def __init__(self, dim, b_g = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_r = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_r = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.w_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.u_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.b_g = b_g # this is used to hack initial bias of the above to be below zero, such that gate is initialized close to identity\n",
    "        self.w_g = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_g = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        r = self.sigmoid(self.w_r(y) + self.u_r(x))\n",
    "        z = self.sigmoid(self.w_z(y) + self.u_z(x) - self.b_g) # when zero, gate passes identity of residual\n",
    "        h_hat = self.tanh(self.w_g(y) + self.u_g(r * x))\n",
    "        g = (1-z)*x + z * h_hat\n",
    "        return g\n",
    "        \n",
    "\n",
    "class mlp(nn.Module):\n",
    "    # 1d temporal convolution\n",
    "    # no communication between tokens, uses same kernel for each token spot\n",
    "    def __init__(self, embed_dim, internal_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(embed_dim, internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(internal_dim, embed_dim)\n",
    "        ) # no second relu at output of mlp\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.block(input)\n",
    "\n",
    "\n",
    "class cross_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "\n",
    "        return self.attention(x, enc, enc)[0]\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.attention(x, x, x)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Smear_key(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "    sequence_length,\n",
    "    heads\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(1, heads, sequence_length - 1, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, k):\n",
    "        itrp = self.sigmoid(self.alpha)\n",
    "        smear = k[:,:,1:,:]*itrp + k[:,:,:-1,:]*(1-itrp)\n",
    "        return torch.cat([k[:,:, 0:1, :], smear], dim = 2)\n",
    "\n",
    "class decoder_mha(nn.Module):\n",
    "    #Masked smeared self attention\n",
    "    def __init__(self, model_dim, sequence_length, heads) -> None:\n",
    "        super().__init__()\n",
    "        self.mask = torch.triu(torch.ones(sequence_length, sequence_length) * float('-inf'), diagonal=1) # make batch, heads, seq,seq\n",
    "        self.model_dim = model_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.heads = heads\n",
    "        self.key_dim = model_dim // heads\n",
    "        self.W_q = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_k = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_v = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.output = nn.Linear(model_dim, model_dim, bias=True)\n",
    "        self.ln = nn.LayerNorm(model_dim)\n",
    "        self.smear = Smear_key(sequence_length, heads)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # batch, sequence, model_dim\n",
    "        q = self.W_q(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        k = self.W_k(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        v = self.W_v(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        k = self.smear(k)\n",
    "        # batch, heads, sequence, dim // heads\n",
    "        key_dim = k.shape[-1:][0]\n",
    "        scores = q @ k.transpose(2,3) / key_dim**.5\n",
    "        scores += self.mask\n",
    "        attn = torch.softmax(scores, dim = 3)\n",
    "        mha = attn @ v\n",
    "        mha = mha.transpose(1, 2).contiguous().view(-1, self.sequence_length, self.model_dim)\n",
    "        out = self.output(mha)\n",
    "        # batch, sequence, model_dim\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # not masked, no cross attention, no memory, for encoder\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = self_attention(\n",
    "            embed_dimension=embed_dim,\n",
    "            num_heads=attention_heads\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim= embed_dim,\n",
    "            internal_dim=mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class decoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # masked, cross attention, smeared key\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_lenth\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = decoder_mha(\n",
    "            model_dim=embed_dim,\n",
    "            sequence_length=sequence_lenth,\n",
    "            heads=attention_heads\n",
    "        ) #smeared key masked self attention\n",
    "\n",
    "        self.cross_mha = cross_attention(\n",
    "            embed_dimension = embed_dim,\n",
    "            num_heads = attention_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim = embed_dim,\n",
    "            internal_dim = mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate3 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "        # masked self attention, smeared key\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "\n",
    "        # cross attention\n",
    "        # consider output sequence length and \n",
    "        y = self.ln1(x)\n",
    "        enc = self.ln1(enc)\n",
    "        y = self.cross_mha(enc, x)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "\n",
    "        # position-wise multi layper perceptron\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding class drawn largely from pytorch tutorial on thier website\n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(sequence_length).unsqueeze(1)\n",
    "        freq = torch.exp(torch.arange(0, model_dim, 2) * (-math.log(10000.0) / model_dim))\n",
    "        pe = torch.zeros(1, sequence_length, model_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * freq)\n",
    "        pe[0, :, 1::2] = torch.cos(position * freq)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # no inductive biases on encoder here\n",
    "        self.block = nn.Sequential()\n",
    "        for x in range(layers):\n",
    "            self.block.append(encoder_layer(\n",
    "                embed_dim = model_dim,\n",
    "                mlp_dim = mlp_dim,\n",
    "                attention_heads = heads,\n",
    "                sequence_length = sequence_length\n",
    "            ))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe = positional_encoding(\n",
    "            model_dim=model_dim, \n",
    "            sequence_length=sequence_length\n",
    "            )\n",
    "\n",
    "        self.block = []\n",
    "\n",
    "        for x in range(layers):\n",
    "            self.block.append(\n",
    "                decoder_layer(\n",
    "                    embed_dim = model_dim,\n",
    "                    mlp_dim= mlp_dim,\n",
    "                    attention_heads= heads,\n",
    "                    sequence_lenth = sequence_length\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # y is input from encoder\n",
    "        x = self.pe(x)\n",
    "        for layer in self.block:\n",
    "            x = layer(x,y)\n",
    "            \n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding class drawn largely from tutorial on pytorch website\n",
    "class positional_encoding(nn.Module):\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(sequence_length).unsqueeze(1)\n",
    "        freq = torch.exp(torch.arange(0, model_dim, 2) * (-math.log(10000.0) / model_dim))\n",
    "        pe = torch.zeros(1, sequence_length, model_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * freq)\n",
    "        pe[0, :, 1::2] = torch.cos(position * freq)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLformer(nn.Module):\n",
    "    #modify to do buffer and positional encoding\n",
    "\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    attn_heads,\n",
    "    sequence_length,\n",
    "    enc_layers,\n",
    "    dec_layers,\n",
    "    action_dim\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder(\n",
    "            layers=enc_layers,\n",
    "            model_dim=model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length = sequence_length\n",
    "        )\n",
    "\n",
    "        self.decoder = decoder(\n",
    "            layers=dec_layers,\n",
    "            model_dim= model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length=sequence_length,\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(model_dim, action_dim),\n",
    "            nn.ReLU(),\n",
    "            grad_skip_softmax() # To do neural replicator dynamics\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(model_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        enc = self.encoder(enc_input)\n",
    "        dec = self.decoder(dec_input, enc)\n",
    "        policy = self.actor(dec)\n",
    "        value = self.critic(dec)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model = RLformer(\n",
    "            model_dim =,\n",
    "            mlp_dim =,\n",
    "            attn_heads =,\n",
    "            sequence_length =,\n",
    "            enc_layers =,\n",
    "            dec_layers =,\n",
    "            action_dim =,\n",
    "        )\n",
    "\n",
    "        self.register_buffer('buffername', tensor=)\n",
    "    \n",
    "    def init_player(self, player, hand):\n",
    "        # initialize this players hand through encoder and store in buffer, after tokenizing\n",
    "\n",
    "    def forward(self, player, obs_flat, reward_flat):\n",
    "        #takes flattened inputs in list form, not tokenized\n",
    "\n",
    "        return action, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "class poker_env():\n",
    "    '''\n",
    "    Texas no-limit holdem environment.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_players) -> None:\n",
    "\n",
    "        self.n_players = n_players\n",
    "\n",
    "        self.stacks = [0] * n_players\n",
    "        for player in range(n_players):\n",
    "            self.stacks[player] = 200\n",
    "\n",
    "        self.button = 0  # button starts at player 0 WLOG\n",
    "\n",
    "        self.deck = []\n",
    "        for suit in [\"hearts\", \"diamonds\", \"spades\", \"clubs\"]:\n",
    "            for rank in range(2, 15):\n",
    "                self.deck += [suit, rank]\n",
    "\n",
    "    def new_hand(self):\n",
    "        self.community_cards = []\n",
    "        self.hands = []\n",
    "        self.deck_position = 0\n",
    "        self.button = (self.button + 1) % self.n_players\n",
    "        self.in_turn = (self.button + 1) % self.n_players\n",
    "        self.behind = [0] * self.n_players\n",
    "        self.in_hand = [True] * self.n_players\n",
    "        self.took_action = [\n",
    "                               False] * self.n_players  # tracks whether players have taken action in a specific round of betting\n",
    "        self.pot = 0\n",
    "        self.current_bet = 0\n",
    "        self.stage = 0  # 0: pre-flop, 1: flop, 2: turn, 3: river\n",
    "        self.deck_position = 0\n",
    "\n",
    "        # deal cards, pass to agents\n",
    "        random.shuffle(self.deck)\n",
    "        for i in range(self.n_players):\n",
    "            self.hands += self.get_next_cards(2)\n",
    "\n",
    "        # big blind is 2, small blind is 1\n",
    "        small_blind = {'player': self.in_turn, 'type': 'bet', 'value': 1}\n",
    "        rewards_1, observations_1 = self.take_action(small_blind)\n",
    "\n",
    "        big_blind_player = self.in_turn\n",
    "        big_blind = {'player': big_blind_player, 'type': 'bet', 'value': 2}\n",
    "        rewards_2, observations_2 = self.take_action(big_blind)\n",
    "        self.took_action[big_blind_player] = False\n",
    "\n",
    "        rewards_1 += rewards_2\n",
    "        observations_1 += observations_2\n",
    "\n",
    "        return rewards_1, observations_1\n",
    "\n",
    "    def get_hand(self, player):\n",
    "        if len(self.hands) == 0:\n",
    "            return None\n",
    "        return self.hands[player]\n",
    "\n",
    "    def take_action(self, action):\n",
    "        '''\n",
    "        Only function that is externally called in training\n",
    "        Takes an action, returns a rewards tensor which has an element for each player, and a list of observations. \n",
    "        Observations are all public information -- does not include dealt hands\n",
    "        Moves game state to next point where action input is required\n",
    "        Rewards implementation currently changing -- very fucked up rn\n",
    "        '''\n",
    "        rewards = [torch.zeros(self.n_players)]\n",
    "\n",
    "        observations = [action]  # first observation returned is always the action being taken\n",
    "        player = action['player']\n",
    "        type = action['type']  # action type is one of {bet, call, fold}\n",
    "        value = action['value']\n",
    "\n",
    "        self.took_action[player] = True\n",
    "\n",
    "        if type == 'bet':\n",
    "            # move money from player to pot\n",
    "            self.stacks[player] -= value\n",
    "            self.pot += value\n",
    "            self.current_bet += value  # bets are valued independently and are NOT measured by cumulative sum -- current_bet tracks that\n",
    "\n",
    "            # reward is negative of amount bet\n",
    "            rewards[0][player] = -value\n",
    "\n",
    "            # other players are now behind the bet\n",
    "            for x in range(self.n_players):\n",
    "                self.behind[x] += value\n",
    "\n",
    "            # player who just bet cannot be behind\n",
    "            self.behind[player] = 0\n",
    "\n",
    "        if type == 'call':\n",
    "            # need to catch up to current bet\n",
    "            call_size = self.behind[player]\n",
    "            self.behind[player] = 0\n",
    "\n",
    "            # move money from player to pot\n",
    "            self.stacks[player] -= call_size\n",
    "            self.pot += call_size\n",
    "            self.current_bet += call_size  # bets are valued independently and are NOT measured by cumulative sum -- current_bet tracks that\n",
    "\n",
    "            # reward is negative of amount bet\n",
    "            rewards[0][player] = -1 * call_size\n",
    "\n",
    "        if type == 'fold':\n",
    "            # player becomes inactive\n",
    "            self.in_hand[player] = False\n",
    "\n",
    "        # if everyone is square or folded, advance to next game stage\n",
    "        square_check = True\n",
    "        for p in range(self.n_players):\n",
    "            if (self.in_hand[p] and self.behind[p] != 0) or not self.took_action[\n",
    "                p]:  # Big blind option handled via took_action\n",
    "                square_check = False\n",
    "\n",
    "        if square_check:\n",
    "            # advance stage, and any other subcalls that come with that\n",
    "            advance_stage_rewards, advance_stage_observations = self.advance_stage()\n",
    "            rewards += advance_stage_rewards\n",
    "            observations += advance_stage_observations\n",
    "\n",
    "        else:\n",
    "            # advance to next player\n",
    "            self.in_turn = (self.in_turn + 1) % self.n_players\n",
    "\n",
    "        return rewards, observations\n",
    "\n",
    "    def advance_stage(self):\n",
    "        # this is called anytime that there is no player who is: 1. in the hand, 2. behind the bet, and 3. has not taken action\n",
    "        advance_stage_rewards = [torch.zeros(self.n_players)]\n",
    "        advance_stage_observations = []\n",
    "\n",
    "        # payout if only one player is left\n",
    "        if sum(self.in_hand) == 1:\n",
    "            for p in range(self.n_players):\n",
    "                if self.in_hand[p]:\n",
    "                    # payout!\n",
    "                    advance_stage_rewards[0][p] += self.pot\n",
    "                    advance_stage_observations += {'player': p, 'type': 'win', 'value': self.pot}\n",
    "            new_hand_rewards, new_hand_observations = self.new_hand()  # move on to next hand\n",
    "            advance_stage_rewards += new_hand_rewards\n",
    "            advance_stage_observations += new_hand_observations\n",
    "\n",
    "        # advance stage if not river\n",
    "        elif self.stage != 3:\n",
    "            self.stage += 1\n",
    "            for p in range(self.n_players):\n",
    "                if self.in_hand[\n",
    "                    p]:  # this keeps took_action true for players who have folded to save a conditional above\n",
    "                    self.took_action[p] = False\n",
    "            advance_stage_rewards, advance_stage_observations = self.card_reveal()\n",
    "\n",
    "        # compare hands and payout, then deal new hand\n",
    "        else:\n",
    "            winners = self.determine_showdown_winners()\n",
    "            for p in winners:\n",
    "                advance_stage_rewards[0][p] += self.pot / len(winners)\n",
    "                advance_stage_observations += {'player': p, 'type': 'win', 'value': self.pot / len(winners)}\n",
    "\n",
    "            new_hand_rewards, new_hand_observations = self.new_hand()  # move on to next hand\n",
    "            advance_stage_rewards += new_hand_rewards\n",
    "            advance_stage_observations += new_hand_observations\n",
    "\n",
    "        return advance_stage_rewards, advance_stage_observations\n",
    "\n",
    "    def card_reveal(self):\n",
    "\n",
    "        if self.stage == 0:\n",
    "            # revealing the flop\n",
    "            card_rewards = [torch.zeros(self.n_players)] * 3  # card reveals have reward zero\n",
    "            card_observations = self.get_next_cards(3)\n",
    "            self.community_cards.extend(card_observations)\n",
    "        else:\n",
    "            # one card to be revealed\n",
    "            card_rewards = torch.zeros(self.n_players)\n",
    "            card_observations = self.get_next_cards(1)\n",
    "            self.community_cards.extend(card_observations)\n",
    "        self.in_turn = (self.button + 1) % self.n_players\n",
    "\n",
    "        return card_rewards, card_observations\n",
    "\n",
    "    def get_next_cards(self, num_cards):\n",
    "        if num_cards == 1:\n",
    "            card = self.deck[self.deck_position]\n",
    "            self.deck_position += 1\n",
    "            return card\n",
    "        elif num_cards == 2:\n",
    "            cards = [self.deck[self.deck_position], self.deck[self.deck_position + 1]]\n",
    "            self.deck_position += 2\n",
    "            return cards\n",
    "        elif num_cards == 3:\n",
    "            cards = [self.deck[self.deck_position], self.deck[self.deck_position + 1],\n",
    "                     self.deck[self.deck_position + 2]]\n",
    "            self.deck_position += 3\n",
    "            return cards\n",
    "        return None\n",
    "\n",
    "    def determine_showdown_winners(self):\n",
    "        scores = [0] * self.n_players\n",
    "        for p in range(self.n_players):\n",
    "            if not self.in_hand[p]:\n",
    "                continue\n",
    "\n",
    "            cards = self.community_cards + self.hands[p]\n",
    "\n",
    "            rank_count = [0] * 13\n",
    "            suit_count = {\"hearts\": 0, \"diamonds\": 0, \"spades\": 0, \"clubs\": 0}\n",
    "\n",
    "            for card in cards:\n",
    "                suit_count[card[0]] += 1\n",
    "                rank_count[card[1]] += 1\n",
    "\n",
    "            # find rank with highest count and rank with second highest count\n",
    "            first_count = 0\n",
    "            second_count = 0\n",
    "            first_rank = 0\n",
    "            second_rank = 0\n",
    "            straight_count = 0\n",
    "            straight_high = 0\n",
    "            for rank in range(2, 15):\n",
    "                current_count = rank_count[rank]\n",
    "                if current_count > first_count:\n",
    "                    second_count = first_count\n",
    "                    second_rank = first_rank\n",
    "                    first_count = current_count\n",
    "                    first_rank = rank\n",
    "                elif current_count == first_count:\n",
    "                    if rank > first_rank:\n",
    "                        second_count = first_count\n",
    "                        second_rank = first_rank\n",
    "                        first_count = current_count\n",
    "                        first_rank = rank\n",
    "                    elif current_count == second_count:\n",
    "                        second_rank = rank\n",
    "                    else:\n",
    "                        second_count = current_count\n",
    "                        second_rank = rank\n",
    "                elif current_count > second_count:\n",
    "                    second_count = rank\n",
    "                    second_rank = rank\n",
    "                elif current_count == second_count:\n",
    "                    second_rank = rank\n",
    "\n",
    "                if current_count == 0:\n",
    "                    continue\n",
    "\n",
    "                if rank == 2:\n",
    "                    if rank_count[14] > 0:\n",
    "                        straight_count = 2\n",
    "                else:\n",
    "                    if rank_count[rank - 1] > 0:\n",
    "                        straight_count += 1\n",
    "                    else:\n",
    "                        straight_count = 1\n",
    "\n",
    "                    if straight_count >= 5:\n",
    "                        straight_high = rank\n",
    "\n",
    "            # check for flush\n",
    "            flush_high = 0\n",
    "            flush_suit = \"\"\n",
    "            for suit in suit_count:\n",
    "                if suit_count[suit] >= 5:\n",
    "                    flush_suit = suit\n",
    "                    for card in cards:\n",
    "                        if card[0] == suit:\n",
    "                            flush_high = max(flush_high, card[1])\n",
    "\n",
    "            # check for straight flush\n",
    "            if flush_high != 0 and straight_high != 0:\n",
    "                sf_high = 0\n",
    "                sf_count = 0\n",
    "                suit_ranks = [0] * 13\n",
    "                for card in cards:\n",
    "                    if card[0] == flush_suit:\n",
    "                        suit_ranks[card[1]] = 1\n",
    "\n",
    "                for rank in range(2, 15):\n",
    "                    if rank == 2:\n",
    "                        if suit_ranks[14]:\n",
    "                            sf_count = 2\n",
    "                        else:\n",
    "                            if suit_ranks[rank - 1]:\n",
    "                                sf_count += 1\n",
    "                            else:\n",
    "                                sf_count = 1\n",
    "\n",
    "                            if sf_count >= 5:\n",
    "                                sf_high = rank\n",
    "\n",
    "                if sf_high != 0:\n",
    "                    scores[p] = 27 + (0.2 * sf_high)\n",
    "                    continue\n",
    "\n",
    "            # quads\n",
    "            if first_count == 4:\n",
    "                scores[p] = 24 + (0.2 * first_rank)\n",
    "                continue\n",
    "\n",
    "            # full house\n",
    "            if first_count == 3 and second_count >= 2:\n",
    "                scores[p] = 21 + 0.2 * first_rank + 0.01 * second_rank\n",
    "                continue\n",
    "\n",
    "            # flush\n",
    "            if flush_high != 0:\n",
    "                scores[p] = 18 + (0.2 * flush_high)\n",
    "                continue\n",
    "\n",
    "            # straight\n",
    "            if straight_high != 0:\n",
    "                scores[p] = 15 + (0.2 * straight_high)\n",
    "                continue\n",
    "\n",
    "            # sort ranks now that high cards matter\n",
    "            ranks = []\n",
    "            for card in cards:\n",
    "                ranks += card[1]\n",
    "            ranks.sort()\n",
    "\n",
    "            # trips\n",
    "            if first_count == 3:\n",
    "                high = 0\n",
    "                second_high = 0\n",
    "                pos = 7\n",
    "                while high == 0 and second_high == 0:\n",
    "                    if ranks[pos] == first_rank:\n",
    "                        pass\n",
    "                    elif high == 0:\n",
    "                        high = ranks[pos]\n",
    "                    else:\n",
    "                        second_high = ranks[pos]\n",
    "                    pos -= 1\n",
    "\n",
    "                scores[p] = 12 + 0.2 * high + 0.01 * second_high\n",
    "                continue\n",
    "\n",
    "            # two pair\n",
    "            if first_count == 2 and second_count == 2:\n",
    "                high = 0\n",
    "                pos = 7\n",
    "                while high == 0:\n",
    "                    if ranks[pos] == first_rank or ranks[pos] == second_rank:\n",
    "                        pass\n",
    "                    else:\n",
    "                        high = ranks[pos]\n",
    "\n",
    "                    pos -= 1\n",
    "\n",
    "                scores[p] = 9 + 0.2 * first_rank + 0.01 * second_rank + 0.002 * high\n",
    "                continue\n",
    "\n",
    "            # pair\n",
    "            if first_count == 2:\n",
    "                high = 0\n",
    "                second_high = 0\n",
    "                third_high = 0\n",
    "                pos = 7\n",
    "                while high == 0 and second_high == 0 and third_high == 0:\n",
    "                    if ranks[pos] == first_rank or ranks[pos] == second_rank:\n",
    "                        pass\n",
    "                    elif high == 0:\n",
    "                        high = ranks[pos]\n",
    "                    elif second_high == 0:\n",
    "                        second_high = ranks[pos]\n",
    "                    else:\n",
    "                        third_high = ranks[pos]\n",
    "                    pos -= 1\n",
    "                scores[p] = 6 + 0.2 * first_rank + 0.01 * high + 0.002 * second_high + 0.0005 * third_high\n",
    "                continue\n",
    "\n",
    "            # high card\n",
    "            scores[p] = 0.2 * ranks[7] + 0.01 * ranks[6] + 0.002 * ranks[5] + 0.0005 * ranks[4] + 0.00001 * ranks[3]\n",
    "            continue\n",
    "\n",
    "        max_score = 0\n",
    "        winners = []\n",
    "        for p in scores:\n",
    "            if scores[p] > max_score:\n",
    "                winners = [p]\n",
    "                max_score = scores[p]\n",
    "            elif scores[p] == max_score:\n",
    "                winners += p\n",
    "\n",
    "        return winners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class actor_critic():\n",
    "    #Needs to be able to run hand, return loss with grad enabled\n",
    "    def __init__(self, \n",
    "    max_sequence: int = 200, \n",
    "    n_players: int = 2,\n",
    "    gamma: float = .8,\n",
    "    n_actions: int = 10, # random placeholder value\n",
    "    ) -> None:\n",
    "        self.gamma = gamma\n",
    "        self.env = poker_env(n_players = n_players)\n",
    "        self.agent = Agent()\n",
    "\n",
    "        self.observations = [] #this will be a list of lists, each is the list of observations in a hand\n",
    "        self.obs_flat = list(chain(*self.observations))\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.rewards_flat = list(chain(*self.rewards))\n",
    "\n",
    "        self.values = []\n",
    "        self.val_flat = list(chain(*self.values))\n",
    "\n",
    "        self.action_log_probabilies = []\n",
    "        self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "        self.max_sequence = max_sequence\n",
    "\n",
    "        self.n_players = n_players\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def init_hands(self):\n",
    "        # get all hands\n",
    "        # run encoder for each of players\n",
    "        for player in range(self.n_players):\n",
    "            hand = self.env.get_hand(player)\n",
    "            self.agent.init_player(player, hand)\n",
    "\n",
    "    \n",
    "    def chop_seq(self):\n",
    "        #if length of observations is above a certain size, chop it back down to under sequence length by removing oldest hand\n",
    "        #return flattened version to give to model on next run\n",
    "        if len(self.observations) > self.max_sequence:\n",
    "            self.observations = self.observations[1:]\n",
    "            self.obs_flat = list(chain(*self.observations))\n",
    "\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.rewards_flat = list(chain(*self.rewards_flat))\n",
    "\n",
    "            self.values = self.values[1:]\n",
    "            self.val_flat = list(chain(*self.values))\n",
    "\n",
    "            self.action_log_probabilies = self.action_log_probabilies[1:]\n",
    "            self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "        else:\n",
    "            self.obs_flat = list(chain(*self.observations))\n",
    "            self.rewards_flat = list(chain(*self.rewards_flat))\n",
    "            self.val_flat = list(chain(*self.values))\n",
    "            self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "    def play_hand(self):\n",
    "        # makes agent play one hand\n",
    "        \n",
    "        # deal cards\n",
    "        rewards, observations = self.env.new_hand() # start a new hand\n",
    "        self.init_hands() # pre load all of the hands\n",
    "\n",
    "        # init lists for this hand\n",
    "        self.observations += [observations] \n",
    "        self.rewards += [rewards]\n",
    "\n",
    "        self.chop_seq() # prepare for input to model\n",
    "        \n",
    "        hand_over = False\n",
    "        while not hand_over:                \n",
    "\n",
    "            # get values and policy -- should be in list form over sequence length\n",
    "            values, policy_dists = self.agent(self.obs_flat, self.rewards_flat)\n",
    "            value = values[-1].detach().numpy()[0,0] # get last value estimate\n",
    "            dist = policy_dists[-1].detach().numpy() # get last distribution\n",
    "\n",
    "            # randomly sample an action\n",
    "            action = np.random.choice(self.n_actions, p=np.squeeze(dist))\n",
    "\n",
    "            # UNFINISHED: Need to detokenize actions HERE\n",
    "\n",
    "            alp = torch.log(policy_dist.squeeze(0)[action])\n",
    "            reward, obs, hand_over = self.env.take_action(action) # need to change environment to return hand_over boolean\n",
    "\n",
    "            # add new information from this step\n",
    "            self.rewards[-1].append(reward)\n",
    "            self.observations[-1].append(obs)\n",
    "            self.values[-1].append(value)\n",
    "            self.action_log_probabilies.append(alp)\n",
    "            \n",
    "            # prepare for next action\n",
    "            self.chop_seq()\n",
    "        \n",
    "        V_T, _ = self.agent(self.obs_flat, self.rewards_flat)\n",
    "        \n",
    "        # process gradients and return loss:\n",
    "        return self.get_loss(V_T)\n",
    "\n",
    "    def get_loss(self, values, rewards, V_T):\n",
    "\n",
    "        Qs = []\n",
    "        Q_t = V_T\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Q_t = rewards[t] + self.gamma * Q_t\n",
    "            Qs[t] = Q_t\n",
    "        \n",
    "        Qs = torch.FloatTensor(Qs)\n",
    "        values = torch.FloatTensor(self.val_flat)\n",
    "        alps = torch.stack(self.alp_flat)\n",
    "        advantages = Qs - values\n",
    "\n",
    "        \n",
    "        actor_loss = (-alps * advantages).mean() # loss function for policy going into softmax on backpass\n",
    "        critic_loss = 0.5 * advantages.pow(2).mean() # autogressive critic loss\n",
    "        loss = actor_loss + critic_loss # no entropy in this since that would deviate from deepnash\n",
    "        return loss\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 0, 5, 0, 8]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward = [0, 2,3,0,5,0,8]\n",
    "reward[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<range_iterator at 0x1209af3f0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed(range(len(rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
