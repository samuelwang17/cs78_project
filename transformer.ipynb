{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grad_skip_softmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sm(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # skip gradient through the softmax on backward pass\n",
    "        return grad\n",
    "        \n",
    "class gru(nn.Module):\n",
    "    # 'gated-recurrent-unit type gating' as seen in GTrXL paper\n",
    "    def __init__(self, dim, b_g = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_r = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_r = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.w_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.u_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.b_g = b_g # this is used to hack initial bias of the above to be below zero, such that gate is initialized close to identity\n",
    "        self.w_g = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_g = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        r = self.sigmoid(self.w_r(y) + self.u_r(x))\n",
    "        z = self.sigmoid(self.w_z(y) + self.u_z(x) - self.b_g) # when zero, gate passes identity of residual\n",
    "        h_hat = self.tanh(self.w_g(y) + self.u_g(r * x))\n",
    "        g = (1-z)*x + z * h_hat\n",
    "        return g\n",
    "        \n",
    "\n",
    "class mlp(nn.Module):\n",
    "    # 1d temporal convolution\n",
    "    # no communication between tokens, uses same kernel for each token spot\n",
    "    def __init__(self, embed_dim, internal_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(embed_dim, internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(internal_dim, embed_dim)\n",
    "        ) # no second relu at output of mlp\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.block(input)\n",
    "\n",
    "\n",
    "class cross_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "\n",
    "        return self.attention(x, enc, enc)[0]\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.attention(x, x, x)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Smear_key(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "    sequence_length,\n",
    "    heads\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(1, heads, sequence_length - 1, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, k):\n",
    "        itrp = self.sigmoid(self.alpha)\n",
    "        smear = k[:,:,1:,:]*itrp + k[:,:,:-1,:]*(1-itrp)\n",
    "        return torch.cat([k[:,:, 0:1, :], smear], dim = 2)\n",
    "\n",
    "class decoder_mha(nn.Module):\n",
    "    #Masked smeared self attention\n",
    "    def __init__(self, model_dim, sequence_length, heads) -> None:\n",
    "        super().__init__()\n",
    "        self.mask = torch.triu(torch.ones(sequence_length, sequence_length) * float('-inf'), diagonal=1) # make batch, heads, seq,seq\n",
    "        self.model_dim = model_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.heads = heads\n",
    "        self.key_dim = model_dim // heads\n",
    "        self.W_q = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_k = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_v = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.output = nn.Linear(model_dim, model_dim, bias=True)\n",
    "        self.ln = nn.LayerNorm(model_dim)\n",
    "        self.smear = Smear_key(sequence_length, heads)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # batch, sequence, model_dim\n",
    "        q = self.W_q(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        k = self.W_k(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        v = self.W_v(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        k = self.smear(k)\n",
    "        # batch, heads, sequence, dim // heads\n",
    "        key_dim = k.shape[-1:][0]\n",
    "        scores = q @ k.transpose(2,3) / key_dim**.5\n",
    "        scores += self.mask\n",
    "        attn = torch.softmax(scores, dim = 3)\n",
    "        mha = attn @ v\n",
    "        mha = mha.transpose(1, 2).contiguous().view(-1, self.sequence_length, self.model_dim)\n",
    "        out = self.output(mha)\n",
    "        # batch, sequence, model_dim\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # not masked, no cross attention, no memory, for encoder\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = self_attention(\n",
    "            embed_dimension=embed_dim,\n",
    "            num_heads=attention_heads\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim= embed_dim,\n",
    "            internal_dim=mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class decoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # masked, cross attention, smeared key\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_lenth\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = decoder_mha(\n",
    "            model_dim=embed_dim,\n",
    "            sequence_length=sequence_lenth,\n",
    "            heads=attention_heads\n",
    "        ) #smeared key masked self attention\n",
    "\n",
    "        self.cross_mha = cross_attention(\n",
    "            embed_dimension = embed_dim,\n",
    "            num_heads = attention_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim = embed_dim,\n",
    "            internal_dim = mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate3 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "        # masked self attention, smeared key\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "\n",
    "        # cross attention\n",
    "        # consider output sequence length and \n",
    "        y = self.ln1(x)\n",
    "        enc = self.ln1(enc)\n",
    "        y = self.cross_mha(enc, x)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "\n",
    "        # position-wise multi layper perceptron\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # no inductive biases on encoder here\n",
    "        self.block = nn.Sequential()\n",
    "        for x in range(layers):\n",
    "            self.block.append(encoder_layer(\n",
    "                embed_dim = model_dim,\n",
    "                mlp_dim = mlp_dim,\n",
    "                attention_heads = heads,\n",
    "                sequence_length = sequence_length\n",
    "            ))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pe = positional_encoding(\n",
    "            model_dim=model_dim, \n",
    "            sequence_length=sequence_length\n",
    "            )\n",
    "\n",
    "        self.block = []\n",
    "\n",
    "        for x in range(layers):\n",
    "            self.block.append(\n",
    "                decoder_layer(\n",
    "                    embed_dim = model_dim,\n",
    "                    mlp_dim= mlp_dim,\n",
    "                    attention_heads= heads,\n",
    "                    sequence_lenth = sequence_length\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # y is input from encoder\n",
    "        x = self.pe(x)\n",
    "        for layer in self.block:\n",
    "            x = layer(x,y)\n",
    "            \n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding class drawn largely from tutorial on pytorch website\n",
    "class positional_encoding(nn.Module):\n",
    "    # tested and functional\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(sequence_length).unsqueeze(1)\n",
    "        freq = torch.exp(torch.arange(0, model_dim, 2) * (-math.log(10000.0) / model_dim))\n",
    "        pe = torch.zeros(1, sequence_length, model_dim)\n",
    "        pe[0, :, 0::2] = torch.sin(position * freq)\n",
    "        pe[0, :, 1::2] = torch.cos(position * freq)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLformer(nn.Module):\n",
    "    # tested and functional\n",
    "\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    attn_heads,\n",
    "    sequence_length,\n",
    "    enc_layers,\n",
    "    dec_layers,\n",
    "    action_dim\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.positional_encoder = positional_encoding(\n",
    "            model_dim=model_dim,\n",
    "            sequence_length = sequence_length\n",
    "        )\n",
    "\n",
    "        self.encoder = encoder(\n",
    "            layers=enc_layers,\n",
    "            model_dim=model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length = sequence_length\n",
    "        )\n",
    "\n",
    "        self.decoder = decoder(\n",
    "            layers=dec_layers,\n",
    "            model_dim= model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length=sequence_length,\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(model_dim, action_dim),\n",
    "            nn.ReLU(),\n",
    "            grad_skip_softmax() # To do neural replicator dynamics\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(model_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "\n",
    "        enc = self.encoder(enc_input)\n",
    "        dec_input = self.positional_encoder(dec_input)\n",
    "        dec = self.decoder(dec_input, enc)\n",
    "        policy = self.actor(dec)\n",
    "        value = self.critic(dec)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self,\n",
    "        model_dim,\n",
    "        mlp_dim,\n",
    "        attn_heads,\n",
    "        sequence_length,\n",
    "        enc_layers,\n",
    "        dec_layers,\n",
    "        action_dim,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = RLformer(\n",
    "            model_dim = model_dim,\n",
    "            mlp_dim = mlp_dim,\n",
    "            attn_heads = attn_heads,\n",
    "            sequence_length = sequence_length,\n",
    "            enc_layers = enc_layers,\n",
    "            dec_layers = dec_layers,\n",
    "            action_dim = action_dim,\n",
    "        )\n",
    "\n",
    "        self.hand_tokenizer = None # hand tokenizer HERE\n",
    "        self.seq_tokenizer = None # sequence tokenizer HERE\n",
    "    \n",
    "    def init_player(self, player, hand):\n",
    "        # initialize this players hand and tokenize it, store it in buffer\n",
    "        hand_tensor = hand_tokenizer(hand)\n",
    "        self.register_buffer(f'hand_{player}', tensor= hand_tensor)\n",
    "\n",
    "    def forward(self, player, obs_flat):\n",
    "        #takes flattened inputs in list form, not tokenized\n",
    "        enc_input = self.get_buffer(f'hand_{player}')\n",
    "        dec_input = seq_tokenizer(obs_flat)\n",
    "        policy, value = self.model(enc_input, dec_input)\n",
    "\n",
    "        return policy, value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pokerenv import poker_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class actor_critic():\n",
    "    #Needs to be able to run hand, return loss with grad enabled\n",
    "    def __init__(self, \n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    enc_layers,\n",
    "    dec_layers,\n",
    "    max_sequence: int = 200, \n",
    "    n_players: int = 2,\n",
    "    gamma: float = .8,\n",
    "    n_actions: int = 10, # random placeholder value\n",
    "    ) -> None:\n",
    "        self.gamma = gamma\n",
    "        self.env = poker_env(n_players = n_players)\n",
    "        self.agent = Agent(\n",
    "            model_dim = model_dim,\n",
    "            mlp_dim = mlp_dim,\n",
    "            attn_head = heads,\n",
    "            sequence_length = max_sequence,\n",
    "            enc_layers = enc_layers,\n",
    "            dec_layers = dec_layers,\n",
    "            action_dim = n_actions,\n",
    "        )\n",
    "\n",
    "        self.observations = [] #this will be a list of lists, each is the list of observations in a hand\n",
    "        self.obs_flat = list(chain(*self.observations))\n",
    "        \n",
    "        self.rewards = []\n",
    "        self.rewards_flat = list(chain(*self.rewards))\n",
    "\n",
    "        self.values = []\n",
    "        self.val_flat = list(chain(*self.values))\n",
    "\n",
    "        self.action_log_probabilies = []\n",
    "        self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "        self.max_sequence = max_sequence\n",
    "\n",
    "        self.n_players = n_players\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        self.detokenize = None #detokenizer HERE\n",
    "\n",
    "    def init_hands(self):\n",
    "        # get all hands\n",
    "        # run encoder for each of players\n",
    "        for player in range(self.n_players):\n",
    "            hand = self.env.get_hand(player)\n",
    "            self.agent.init_player(player, hand)\n",
    "    \n",
    "    def chop_seq(self):\n",
    "        #if length of observations is above a certain size, chop it back down to under sequence length by removing oldest hand\n",
    "        #return flattened version to give to model on next run\n",
    "        if len(self.observations) > self.max_sequence:\n",
    "            self.observations = self.observations[1:]\n",
    "            self.obs_flat = list(chain(*self.observations))\n",
    "\n",
    "            self.rewards = self.rewards[1:]\n",
    "            self.rewards_flat = list(chain(*self.rewards_flat))\n",
    "\n",
    "            self.values = self.values[1:]\n",
    "            self.val_flat = list(chain(*self.values))\n",
    "\n",
    "            self.action_log_probabilies = self.action_log_probabilies[1:]\n",
    "            self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "        else:\n",
    "            self.obs_flat = list(chain(*self.observations))\n",
    "            self.rewards_flat = list(chain(*self.rewards_flat))\n",
    "            self.val_flat = list(chain(*self.values))\n",
    "            self.alp_flat = list(chain(*self.action_log_probabilies))\n",
    "\n",
    "    def play_hand(self):\n",
    "        # makes agent play one hand\n",
    "        \n",
    "        # deal cards\n",
    "        rewards, observations = self.env.new_hand() # start a new hand\n",
    "        self.init_hands() # pre load all of the hands\n",
    "\n",
    "        # init lists for this hand\n",
    "        self.observations += [observations] \n",
    "        self.rewards += [rewards]\n",
    "\n",
    "        self.chop_seq() # prepare for input to model\n",
    "        \n",
    "        hand_over = False\n",
    "        while not hand_over:                \n",
    "\n",
    "            # get values and policy -- should be in list form over sequence length\n",
    "            policy, values = self.agent(self.obs_flat)\n",
    "            value = values[-1].detach().numpy()[0,0] # get last value estimate\n",
    "            dist = policy[-1].detach().numpy() # get last policy distribution\n",
    "\n",
    "            # randomly sample an action\n",
    "            action = np.random.choice(self.n_actions, p=np.squeeze(dist))\n",
    "\n",
    "            # UNFINISHED: Need to detokenize actions HERE\n",
    "            action = self.detokenize(action)\n",
    "\n",
    "            alp = torch.log(policy.squeeze(0)[action])\n",
    "            reward, obs, hand_over = self.env.take_action(action) # need to change environment to return hand_over boolean\n",
    "\n",
    "            # add new information from this step\n",
    "            self.rewards[-1].append(reward)\n",
    "            self.observations[-1].append(obs)\n",
    "            self.values[-1].append(value)\n",
    "            self.action_log_probabilies.append(alp)\n",
    "            \n",
    "            # prepare for next action\n",
    "            self.chop_seq()\n",
    "        \n",
    "        V_T, _ = self.agent(self.obs_flat)\n",
    "        \n",
    "        # process gradients and return loss:\n",
    "        return self.get_loss(V_T)\n",
    "\n",
    "    def get_loss(self, values, rewards, V_T):\n",
    "\n",
    "        Qs = []\n",
    "        Q_t = V_T\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Q_t = rewards[t] + self.gamma * Q_t\n",
    "            Qs[t] = Q_t\n",
    "        \n",
    "        Qs = torch.FloatTensor(Qs)\n",
    "        values = torch.FloatTensor(self.val_flat)\n",
    "        alps = torch.stack(self.alp_flat)\n",
    "        advantages = Qs - values\n",
    "\n",
    "        \n",
    "        actor_loss = (-alps * advantages).mean() # loss function for policy going into softmax on backpass\n",
    "        critic_loss = 0.5 * advantages.pow(2).mean() # autogressive critic loss - MSE\n",
    "        loss = actor_loss + critic_loss # no entropy in this since that would deviate from deepnash\n",
    "        return loss\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenize\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(36, model_dim), # tokenizer has 36 dimensional output\n",
    "            nn.ReLU() # allows feature superposition in embedding\n",
    "        )\n",
    "\n",
    "    def tokenize_list(self, observations):\n",
    "        # convert list of observations to 2d tensor\n",
    "        seq = []\n",
    "        for obs in observations:\n",
    "            seq.append(tokenize(obs))\n",
    "        \n",
    "        obs_tensor = torch.stack(seq) #sequence, model_dim\n",
    "        return obs_tensor\n",
    "    \n",
    "    def forward(self, observations):\n",
    "        obs_tensor = self.tokenize_list(observations)\n",
    "        return self.embedding(obs_tensor) # sequence, model_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = poker_env(n_players=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'player': 3, 'type': 'bet', 'value': 1, 'pot': 0, 'p1': 200, 'p2': 198, 'p3': 199, 'p4': 198, 'p5': 200, 'p6': 200, '6': 200}, {'player': 4, 'type': 'bet', 'value': 2, 'pot': 1, 'p1': 200, 'p2': 198, 'p3': 199, 'p4': 197, 'p5': 200, 'p6': 200, '6': 200}]\n"
     ]
    }
   ],
   "source": [
    "rewards, obs = env.new_hand()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = {\n",
    "    'player': 1,\n",
    "    'type': 'bet',\n",
    "    'value': 2\n",
    "}\n",
    "rewards_1, obs_1 = env.take_action(action)\n",
    "rewards += rewards_1\n",
    "obs += obs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytok = Tokenizer(model_dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = mytok(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
