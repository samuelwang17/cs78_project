{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grad_skip_softmax(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.sm = nn.Softmax()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sm(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # skip gradient through the softmax on backward pass\n",
    "        return grad\n",
    "        \n",
    "class gru(nn.Module):\n",
    "    # 'gated-recurrent-unit type gating' as seen in GTrXL paper\n",
    "    def __init__(self, dim, b_g = 1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.w_r = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_r = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.w_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.u_z = nn.Linear(dim, dim, bias = True)\n",
    "        self.b_g = b_g # this is used to hack initial bias of the above to be below zero, such that gate is initialized close to identity\n",
    "        self.w_g = nn.Linear(dim, dim, bias = False)\n",
    "        self.u_g = nn.Linear(dim, dim, bias = False)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        r = self.sigmoid(self.w_r(y) + self.u_r(x))\n",
    "        z = self.sigmoid(self.w_z(y) + self.u_z(x) - self.b_g) # when zero, gate passes identity of residual\n",
    "        h_hat = self.tanh(self.w_g(y) + self.u_g(r * x))\n",
    "        g = (1-z)*x + z * h_hat\n",
    "        return g\n",
    "        \n",
    "\n",
    "class mlp(nn.Module):\n",
    "    # 1d temporal convolution\n",
    "    # no communication between tokens, uses same kernel for each token spot\n",
    "    def __init__(self, embed_dim, internal_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(embed_dim, internal_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(internal_dim, embed_dim)\n",
    "        ) # no second relu at output of mlp\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.block(input)\n",
    "\n",
    "\n",
    "class cross_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "\n",
    "        return self.attention(x, enc, enc)[0]\n",
    "\n",
    "class self_attention(nn.Module):\n",
    "    def __init__(self, embed_dimension, num_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dimension,\n",
    "            num_heads=num_heads,\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.attention(x, x, x)[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Smear_key(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "    sequence_length,\n",
    "    heads\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(1, heads, sequence_length - 1, 1))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, k):\n",
    "        itrp = self.sigmoid(self.alpha)\n",
    "        smear = k[:,:,1:,:]*itrp + k[:,:,:-1,:]*(1-itrp)\n",
    "        return torch.cat([k[:,:, 0:1, :], smear], dim = 2)\n",
    "\n",
    "class decoder_mha(nn.Module):\n",
    "    #Masked smeared self attention\n",
    "    def __init__(self, model_dim, sequence_length, heads) -> None:\n",
    "        super().__init__()\n",
    "        self.mask = torch.triu(torch.ones(sequence_length, sequence_length) * float('-inf'), diagonal=1) # make batch, heads, seq,seq\n",
    "        self.model_dim = model_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.heads = heads\n",
    "        self.key_dim = model_dim // heads\n",
    "        self.W_q = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_k = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.W_v = nn.Linear(model_dim, model_dim, bias=False)\n",
    "        self.output = nn.Linear(model_dim, model_dim, bias=True)\n",
    "        self.ln = nn.LayerNorm(model_dim)\n",
    "        self.smear = Smear_key(sequence_length, heads)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # batch, sequence, model_dim\n",
    "        q = self.W_q(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        k = self.W_k(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        v = self.W_v(x).view(-1, self.sequence_length, self.heads, self.key_dim).transpose(1,2)\n",
    "        k = self.smear(k)\n",
    "        # batch, heads, sequence, dim // heads\n",
    "        key_dim = k.shape[-1:][0]\n",
    "        scores = q @ k.transpose(2,3) / key_dim**.5\n",
    "        scores += self.mask\n",
    "        attn = torch.softmax(scores, dim = 3)\n",
    "        mha = attn @ v\n",
    "        mha = mha.transpose(1, 2).contiguous().view(-1, self.sequence_length, self.model_dim)\n",
    "        out = self.output(mha)\n",
    "        # batch, sequence, model_dim\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # not masked, no cross attention, no memory, for encoder\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = self_attention(\n",
    "            embed_dimension=embed_dim,\n",
    "            num_heads=attention_heads\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim= embed_dim,\n",
    "            internal_dim=mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class decoder_layer(nn.Module):\n",
    "    # transformer layer\n",
    "    # masked, cross attention, smeared key\n",
    "    def __init__(self,\n",
    "    embed_dim,\n",
    "    mlp_dim,\n",
    "    attention_heads,\n",
    "    sequence_lenth\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = decoder_mha(\n",
    "            model_dim=embed_dim,\n",
    "            sequence_length=sequence_lenth,\n",
    "            heads=attention_heads\n",
    "        ) #smeared key masked self attention\n",
    "\n",
    "        self.cross_mha = cross_attention(\n",
    "            embed_dimension = embed_dim,\n",
    "            num_heads = attention_heads,\n",
    "        )\n",
    "\n",
    "        self.mlp = mlp(\n",
    "            embed_dim = embed_dim,\n",
    "            internal_dim = mlp_dim\n",
    "        )\n",
    "\n",
    "        self.gate1 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate2 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "        self.gate3 = gru(\n",
    "            dim = embed_dim\n",
    "        )\n",
    "\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x, enc):\n",
    "        # masked self attention, smeared key\n",
    "        y = self.ln1(x)\n",
    "        y = self.mha(y)\n",
    "        x = self.gate1(x,self.activation(y))\n",
    "\n",
    "        # cross attention\n",
    "        # consider output sequence length and \n",
    "        y = self.ln1(x)\n",
    "        enc = self.ln1(enc)\n",
    "        y = self.cross_mha(enc, x)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "\n",
    "        # position-wise multi layper perceptron\n",
    "        y = self.ln1(x)\n",
    "        y = self.mlp(y)\n",
    "        x = self.gate2(x, self.activation(y))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # no inductive biases on encoder here\n",
    "        self.block = nn.Sequential()\n",
    "        for x in range(layers):\n",
    "            self.block.append(encoder_layer(\n",
    "                embed_dim = model_dim,\n",
    "                mlp_dim = mlp_dim,\n",
    "                attention_heads = heads,\n",
    "                sequence_length = sequence_length\n",
    "            ))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "    layers,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    heads,\n",
    "    sequence_length\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.block = []\n",
    "\n",
    "        for x in range(layers):\n",
    "            self.block.append(\n",
    "                decoder_layer(\n",
    "                    embed_dim = model_dim,\n",
    "                    mlp_dim= mlp_dim,\n",
    "                    attention_heads= heads,\n",
    "                    sequence_lenth = sequence_length\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # y is input from encoder\n",
    "        for layer in self.block:\n",
    "            x = layer(x,y)\n",
    "            \n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "    model_dim,\n",
    "    mlp_dim,\n",
    "    attn_heads,\n",
    "    sequence_length,\n",
    "    enc_layers,\n",
    "    dec_layers,\n",
    "    action_dim\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder(\n",
    "            layers=enc_layers,\n",
    "            model_dim=model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length = sequence_length\n",
    "        )\n",
    "\n",
    "        self.decoder = decoder(\n",
    "            layers=dec_layers,\n",
    "            model_dim= model_dim,\n",
    "            mlp_dim=mlp_dim,\n",
    "            heads=attn_heads,\n",
    "            sequence_length=sequence_length,\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(model_dim, action_dim),\n",
    "            nn.ReLU(),\n",
    "            grad_skip_softmax() # To do neural replicator dynamics\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(model_dim, mlp_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mlp_dim, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, enc_input, dec_input):\n",
    "        enc = self.encoder(enc_input)\n",
    "        dec = self.decoder(dec_input, enc)\n",
    "        policy = self.actor(dec)\n",
    "        value = self.critic(dec)\n",
    "        return policy, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "myrl = RLformer(\n",
    "    model_dim = 10,\n",
    "    mlp_dim = 20,\n",
    "    attn_heads = 2,\n",
    "    sequence_length = 15,\n",
    "    enc_layers = 2,\n",
    "    dec_layers = 2,\n",
    "    action_dim = 4 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs are batch, sequence, model_dim\n",
    "enc_input = torch.rand((8,15,10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = torch.rand((8,15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 15, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/0v1g6ztj2_3272nwsptb6rpc0000gn/T/ipykernel_23193/2946466150.py:7: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.sm(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.97 ms ± 57.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit policy, value = myrl(enc_input, dec_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 15, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.size()\n",
    "#batch,sequence,action space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 15, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = torch.rand((8,5,10))\n",
    "test_mha = decoder_mha(model_dim=10,sequence_length=5,heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4197, -0.1550,  0.1928,  0.2340,  0.2418, -0.2448, -0.0458,\n",
      "          -0.4462,  0.1150, -0.1603],\n",
      "         [-0.4327, -0.1636,  0.1269,  0.3228,  0.1293, -0.1570, -0.0724,\n",
      "          -0.3111,  0.1123, -0.1597],\n",
      "         [-0.4306, -0.1590,  0.1207,  0.3382,  0.0930, -0.1776, -0.1141,\n",
      "          -0.2811,  0.1077, -0.1790],\n",
      "         [-0.3912, -0.1655,  0.1378,  0.3009,  0.0629, -0.2122, -0.1494,\n",
      "          -0.2687,  0.0973, -0.1934],\n",
      "         [-0.3691, -0.1809,  0.1349,  0.2907,  0.0484, -0.2170, -0.1633,\n",
      "          -0.2540,  0.1040, -0.1880]],\n",
      "\n",
      "        [[-0.2681, -0.1029,  0.0621,  0.2428,  0.2743, -0.2279,  0.0875,\n",
      "          -0.4397,  0.2145,  0.0331],\n",
      "         [-0.2520, -0.0825,  0.0711,  0.1987,  0.1775, -0.2456, -0.0125,\n",
      "          -0.3249,  0.1228, -0.0285],\n",
      "         [-0.2599, -0.1167,  0.1054,  0.2196,  0.1694, -0.2229, -0.0119,\n",
      "          -0.3411,  0.1611, -0.0308],\n",
      "         [-0.3217, -0.1924,  0.1413,  0.2453,  0.1684, -0.2055, -0.0476,\n",
      "          -0.3723,  0.1739, -0.0962],\n",
      "         [-0.3469, -0.2085,  0.1293,  0.2762,  0.1758, -0.2221, -0.0695,\n",
      "          -0.3935,  0.1882, -0.1248]],\n",
      "\n",
      "        [[-0.2653, -0.2033,  0.0476,  0.2123,  0.2793, -0.1248, -0.0508,\n",
      "          -0.3529,  0.1827, -0.1065],\n",
      "         [-0.4857, -0.2445,  0.0366,  0.4220,  0.0849, -0.1262, -0.0841,\n",
      "          -0.2931,  0.1966, -0.1292],\n",
      "         [-0.4951, -0.2713,  0.0856,  0.4397,  0.0728, -0.2355, -0.1258,\n",
      "          -0.3560,  0.2076, -0.1850],\n",
      "         [-0.4343, -0.2116,  0.0912,  0.3595,  0.0932, -0.2224, -0.1150,\n",
      "          -0.3138,  0.1478, -0.1739],\n",
      "         [-0.3760, -0.1876,  0.1067,  0.3024,  0.1207, -0.2071, -0.0999,\n",
      "          -0.3166,  0.1511, -0.1454]],\n",
      "\n",
      "        [[-0.2574, -0.2126,  0.2488,  0.1146,  0.0309, -0.4907, -0.1628,\n",
      "          -0.4049,  0.0631, -0.1993],\n",
      "         [-0.3041, -0.2120,  0.1837,  0.2142,  0.0175, -0.3929, -0.1540,\n",
      "          -0.3406,  0.1066, -0.1741],\n",
      "         [-0.2387, -0.1724,  0.1179,  0.1823,  0.0664, -0.2780, -0.0961,\n",
      "          -0.2886,  0.1175, -0.0959],\n",
      "         [-0.2306, -0.1889,  0.1489,  0.1433,  0.1323, -0.2800, -0.1118,\n",
      "          -0.3337,  0.1068, -0.1430],\n",
      "         [-0.2347, -0.2032,  0.1678,  0.1480,  0.1252, -0.2531, -0.1061,\n",
      "          -0.3430,  0.1359, -0.1270]],\n",
      "\n",
      "        [[-0.2155, -0.1416,  0.1123,  0.1938,  0.2319, -0.1972, -0.0172,\n",
      "          -0.3562,  0.2062, -0.0105],\n",
      "         [-0.1879, -0.1861,  0.1432,  0.1750,  0.1507, -0.1672, -0.0555,\n",
      "          -0.3158,  0.1946, -0.0493],\n",
      "         [-0.2488, -0.1683,  0.1778,  0.2053,  0.1448, -0.2134, -0.0739,\n",
      "          -0.3448,  0.1687, -0.1011],\n",
      "         [-0.2865, -0.1596,  0.1366,  0.2586,  0.1554, -0.1914, -0.0450,\n",
      "          -0.3441,  0.1883, -0.0755],\n",
      "         [-0.3190, -0.1983,  0.1387,  0.2707,  0.1559, -0.2043, -0.0663,\n",
      "          -0.3631,  0.1895, -0.1106]],\n",
      "\n",
      "        [[-0.2061, -0.1621,  0.1604,  0.1426, -0.0356, -0.2811, -0.2361,\n",
      "          -0.1660,  0.0788, -0.1395],\n",
      "         [-0.3004, -0.2137,  0.0992,  0.2600,  0.1076, -0.2178, -0.1541,\n",
      "          -0.2958,  0.1832, -0.1349],\n",
      "         [-0.3137, -0.2400,  0.1141,  0.2630,  0.1345, -0.2225, -0.1511,\n",
      "          -0.3274,  0.1821, -0.1616],\n",
      "         [-0.3255, -0.2129,  0.1047,  0.2523,  0.1466, -0.2322, -0.1395,\n",
      "          -0.3176,  0.1306, -0.1777],\n",
      "         [-0.3106, -0.2046,  0.1440,  0.2176,  0.1715, -0.2666, -0.1468,\n",
      "          -0.3550,  0.1226, -0.1993]],\n",
      "\n",
      "        [[-0.3431, -0.1147,  0.1340,  0.3437, -0.0514, -0.2972, -0.1889,\n",
      "          -0.2181,  0.1110, -0.1731],\n",
      "         [-0.3038, -0.1388,  0.1873,  0.2324,  0.0178, -0.2427, -0.1828,\n",
      "          -0.2278,  0.0525, -0.2184],\n",
      "         [-0.2771, -0.1776,  0.1945,  0.1946,  0.0840, -0.2307, -0.1485,\n",
      "          -0.2859,  0.0790, -0.2022],\n",
      "         [-0.2725, -0.1676,  0.1795,  0.1921,  0.0253, -0.2601, -0.1659,\n",
      "          -0.2367,  0.0575, -0.1854],\n",
      "         [-0.2396, -0.1433,  0.1781,  0.1458,  0.0818, -0.2563, -0.1257,\n",
      "          -0.2662,  0.0535, -0.1563]],\n",
      "\n",
      "        [[-0.4222, -0.4576,  0.1674,  0.3255, -0.0765, -0.3207, -0.3159,\n",
      "          -0.2948,  0.1320, -0.3554],\n",
      "         [-0.2604, -0.2769,  0.1427,  0.2285,  0.0600, -0.1776, -0.1319,\n",
      "          -0.2801,  0.1883, -0.1284],\n",
      "         [-0.2767, -0.1892,  0.1344,  0.2214,  0.0908, -0.1914, -0.1016,\n",
      "          -0.2740,  0.1444, -0.1112],\n",
      "         [-0.2776, -0.2280,  0.1502,  0.2331,  0.0900, -0.2073, -0.1184,\n",
      "          -0.3091,  0.1833, -0.1215],\n",
      "         [-0.2980, -0.1963,  0.1122,  0.2503,  0.0740, -0.1974, -0.0947,\n",
      "          -0.2738,  0.1619, -0.0909]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y= test_mha(dec_input)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
